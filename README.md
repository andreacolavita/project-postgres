# Project 1: Data Modeling with Postgres

In this project we are helping our analytics team of *Sparkify* startup to analyze what song users are listening to.
We are creating a ```sparkifybd``` in Postgres to model song and log datasets, reading from JSON files using a start schema optimised for queries on song play analysis.

## Data Source and Data Flow

### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

Below is an example of what a single song file looks like:

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
``` 

### Log Dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

### Data Flow

Processed data derived from the JSON log files, ```data/log_data```, is used to populate **time** and **users** tables. A ```SELECT``` query collects song and artist id from the ***songs*** and ***artists*** tables and combines this with log file derived data to populate the songplays fact table.

## Schema design and ETL pipeline

Our Star Schema has 1 Fact Table (songplays) and 4 Dimenion tables (songs, artists, users, time). 
In the **sql_queries.py** file are defined all the queries to ```DROP```, ```CREATE```, ```INSERT``` and ```SELECT``` tables and in **create_tables.py** are defined the functions to create the database and create and drop the tables each time that we run the ETL pipeline. 

![Data Model diagram](/data/images/DataModel.png)

In **etl.ipynb** there is the Jupyter Notebook used to build the ETL code testing it while developing. In **etl.py** there is the python file used to build the ETL pipeline with the code already developed in the Jupyter Notebook to extract, transform and load the data coming from JSON log files (```data/log_data```) and to store it into Postgres db (```sparkifybd```).
